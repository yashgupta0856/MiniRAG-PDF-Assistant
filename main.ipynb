{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af4d2efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf8155d",
   "metadata": {},
   "source": [
    "**The Document Location**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "623601b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdfPath = \"pdfContent/document.pdf\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb8a786",
   "metadata": {},
   "source": [
    "**Function for loading the file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f796531",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadFile(path):\n",
    "    reader = PdfReader(path)\n",
    "    pages  = []\n",
    "   \n",
    "\n",
    "    for i , page in enumerate(reader.pages):\n",
    "        text = page.extract_text()\n",
    "        pages.append({\"page\": i+ 1,\"text\":text})\n",
    "\n",
    "    return pages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057172d3",
   "metadata": {},
   "source": [
    "**Splitting data into chunks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3d960a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitIntoChunks(pages,chunkSize=1000,overlap=200):\n",
    "    chunks = []\n",
    "    metadatas = []\n",
    "\n",
    "    global_chunk_index = 0\n",
    "\n",
    "    for p in pages:\n",
    "        text = p['text']\n",
    "        page_no = p['page']\n",
    "        text_length = len(text)\n",
    "        start = 0\n",
    "\n",
    "        while start < text_length:\n",
    "            end = start + chunkSize\n",
    "            chunk = text[start:end]\n",
    "            chunk_id  =f\"p{page_no}-c{global_chunk_index}\"\n",
    "            meta = {\n",
    "                \"page\": page_no,\n",
    "                \"start_char\": start,\n",
    "                \"end_char\": min(end, text_length),\n",
    "                \"chunk_id\": chunk_id\n",
    "            }\n",
    "\n",
    "            chunks.append(chunk)\n",
    "            metadatas.append(meta)\n",
    "\n",
    "            global_chunk_index += 1\n",
    "            start = end - overlap\n",
    "\n",
    "    return chunks,metadatas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "67634248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total pages :  6\n",
      "total chunks created :  15\n"
     ]
    }
   ],
   "source": [
    "pages = loadFile(pdfPath)\n",
    "print(\"total pages : \",len(pages))\n",
    "\n",
    "chunks,metadatas = splitIntoChunks(pages)\n",
    "print(\"total chunks created : \",len(chunks))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b519a296",
   "metadata": {},
   "source": [
    "**Loading Embedding Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e005d220",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\YASH\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\YASH\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196af7a9",
   "metadata": {},
   "source": [
    "**ChromaDB setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0689dd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "\n",
    "# creating chromadb instance which will store all chunks and embeddings in it\n",
    "\n",
    "chroma_client = chromadb.Client()\n",
    "collection  = chroma_client.create_collection(name = \"pdf_reader\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafd0ced",
   "metadata": {},
   "source": [
    "**Inserting data into CHROMADB**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "29de9b27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vector db is ready\n"
     ]
    }
   ],
   "source": [
    "for chunk, meta in zip(chunks, metadatas):\n",
    "    emb = model.encode([chunk])[0].tolist()\n",
    "    collection.add(\n",
    "        documents=[chunk],\n",
    "        embeddings=[emb],\n",
    "        ids=[meta[\"chunk_id\"]],\n",
    "        metadatas=[meta]\n",
    "    )\n",
    "\n",
    "print(\"vector db is ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3ca0d3",
   "metadata": {},
   "source": [
    "### Integrating LLM through Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e3a7721",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "def runLLM(prompt):\n",
    "    response = ollama.generate(\n",
    "        # model = \"tinyllama\", # you have to pull \"ollama pull tinyllama\" on your local ollama terminal\n",
    "        model = \"mistral\", # you have to pull \"ollama pull mistral\" on your local ollama terminal\n",
    "        prompt = prompt\n",
    "    )\n",
    "    return response[\"response\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc42e68b",
   "metadata": {},
   "source": [
    "**RAG Function**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc31222c",
   "metadata": {},
   "source": [
    "*Re-Ranking*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5eaaed65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "reranker = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb8f606",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag(query):\n",
    "    print(\"The Question is:\", query)\n",
    "\n",
    "    # STEP 1 — Expand & embed query\n",
    "    expanded = f\"Explain in detail: {query} (in context of armwrestling)\"\n",
    "    query_emb = model.encode([expanded])[0].tolist()\n",
    "\n",
    "    # STEP 2 — Retrieve top K documents (with metadata)\n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_emb],\n",
    "        n_results=5,\n",
    "        include=[\"documents\", \"metadatas\"]\n",
    "    )\n",
    "\n",
    "    retrieved_chunks = results[\"documents\"][0]\n",
    "    retrieved_metas  = results[\"metadatas\"][0]\n",
    "\n",
    "    # STEP 3 — Re-ranking with CrossEncoder\n",
    "    pairs = [(query, chunk) for chunk in retrieved_chunks]\n",
    "    scores = reranker.predict(pairs)\n",
    "\n",
    "    # zip scores + chunks + metadata together for safe sorting\n",
    "    ranked = sorted(\n",
    "        zip(scores, retrieved_chunks, retrieved_metas),\n",
    "        reverse=True,\n",
    "        key=lambda x: x[0]\n",
    "    )\n",
    "\n",
    "    # STEP 4 — Take best top 2\n",
    "    final = ranked[:2]\n",
    "    final_chunks = [x[1] for x in final]\n",
    "    final_metas  = [x[2] for x in final]\n",
    "    final_scores = [float(x[0]) for x in final]\n",
    "\n",
    "    # STEP 5 — Build context\n",
    "    context = \"\\n\\n\".join(final_chunks)\n",
    "\n",
    "    # STEP 6 — Strict No-Hallucination Prompt\n",
    "    prompt = f\"\"\"\n",
    "You are a STRICT RAG assistant. Follow the rules exactly.\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUESTION:\n",
    "{query}\n",
    "\n",
    "IMPORTANT ANSWERING RULES:\n",
    "1. Use ONLY the text in the provided context.\n",
    "2. Do NOT guess, assume, or infer anything outside the context.\n",
    "3. If the exact term is NOT mentioned in the context, say:\n",
    "   \"The term '{query}' is not explicitly mentioned in the document.\"\n",
    "4. If related concepts ARE described, summarize ONLY those parts.\n",
    "5. Keep the answer factual and concise.\n",
    "6. Never use phrases like “it is implied”.\n",
    "\"\"\"\n",
    "\n",
    "    # STEP 7 — Run LLM\n",
    "    answer = runLLM(prompt)\n",
    "\n",
    "    # STEP 8 — Return sources for frontend\n",
    "    sources = []\n",
    "    for meta, score in zip(final_metas, final_scores):\n",
    "        sources.append({\n",
    "            \"chunk_id\": meta.get(\"chunk_id\"),\n",
    "            \"page\": meta.get(\"page\"),\n",
    "            \"start_char\": meta.get(\"start_char\"),\n",
    "            \"end_char\": meta.get(\"end_char\"),\n",
    "            \"score\": score\n",
    "        })\n",
    "\n",
    "    return answer, sources\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0b82d4",
   "metadata": {},
   "source": [
    "**Asking question from user and pass it to the LLM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ae88d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Question is :  what kind of common injuries happen in armwrestling ? \n",
      "Answer:\n",
      "\n",
      " Common Injuries: Distal humerus (upper arm) fractures, elbow/biceps tendon strains, wrist ligament and carpal injuries, shoulder strains.\n"
     ]
    }
   ],
   "source": [
    "# userQuery = input(\"What's your question ?\")\n",
    "# rag(userQuery)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3bfff6",
   "metadata": {},
   "source": [
    "**FASTAPI Deployment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c385a668",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, Form\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "import uvicorn\n",
    "import threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e8266b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = FastAPI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb7274e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allow frontend to call backend ( cors )\n",
    "\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],\n",
    "    allow_credentials=True,\n",
    "    allow_headers = [\"*\"],\n",
    "    allow_methods= [\"*\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5ce3a86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.post(\"/ask\")\n",
    "async def askQuestion(question: str = Form(...)):\n",
    "    answer, sources = rag(question)         \n",
    "    return {\n",
    "        \"answer\": answer,\n",
    "        \"sources\": sources\n",
    "    }\n",
    "\n",
    "\n",
    "@app.get(\"/\")\n",
    "async def home():\n",
    "    return {\"message\":\"RAG Backend is running\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78daa8b",
   "metadata": {},
   "source": [
    "**Running the FastAPI server from notebook**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570ea9ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [11008]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://127.0.0.1:5173 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:63385 - \"GET / HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:63385 - \"GET /favicon.ico HTTP/1.1\" 404 Not Found\n",
      "INFO:     127.0.0.1:51295 - \"GET /ask HTTP/1.1\" 405 Method Not Allowed\n",
      "INFO:     127.0.0.1:51295 - \"GET / HTTP/1.1\" 200 OK\n",
      "The Question is :  what kind of common injuries happen in armwrestling ? \n",
      "Answer:\n",
      "\n",
      " Common injuries in armwrestling include distal humerus (upper arm) fractures, elbow/biceps tendon strains, wrist ligament and carpal injuries, and shoulder strains.\n",
      "INFO:     127.0.0.1:55006 - \"POST /ask HTTP/1.1\" 200 OK\n",
      "The Question is :  what kind of common injuries happen in armwrestling ? \n",
      "Answer:\n",
      "\n",
      " Common Injuries in Armwrestling include: Distal humerus (upper arm) fractures, elbow/biceps tendon strains, wrist ligament and carpal injuries, and shoulder strains.\n",
      "INFO:     127.0.0.1:64005 - \"POST /ask HTTP/1.1\" 200 OK\n",
      "The Question is :  what kind of common injuries happen in armwrestling ? \n",
      "INFO:     127.0.0.1:61767 - \"POST /ask HTTP/1.1\" 200 OK\n",
      "The Question is :  from which country devon larrant belongs to ? \n",
      "INFO:     127.0.0.1:60358 - \"POST /ask HTTP/1.1\" 200 OK\n",
      "The Question is :  from which country devon larrant belongs to ? \n",
      "INFO:     127.0.0.1:56668 - \"POST /ask HTTP/1.1\" 200 OK\n",
      "The Question is :  who is john cena :\n",
      "INFO:     127.0.0.1:64633 - \"POST /ask HTTP/1.1\" 200 OK\n",
      "The Question is :  what is press in armwrestling ?\n",
      "INFO:     127.0.0.1:51696 - \"POST /ask HTTP/1.1\" 200 OK\n",
      "The Question is :  what is press in armwrestling in detail?\n",
      "INFO:     127.0.0.1:52843 - \"POST /ask HTTP/1.1\" 200 OK\n"
     ]
    }
   ],
   "source": [
    "def runServer():\n",
    "    uvicorn.run(app, host=\"127.0.0.1\", port=5173)\n",
    "\n",
    "threading.Thread(target=runServer,daemon=True).start()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
