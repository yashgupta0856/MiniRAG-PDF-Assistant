# MiniRAG PDF Question-Answering System

A lightweight Retrieval-Augmented Generation (RAG) pipeline implemented entirely inside a  **single Jupyter notebook (`notebook.ipynb`)** , allowing users to upload any PDF and ask context-aware questions about its content. This system demonstrates the fundamentals of RAG using:

* **Python** for preprocessing and orchestration
* **PyPDF2** for PDF text extraction
* **SentenceTransformers (MiniLM-L6-v2)** for semantic embeddings
* **ChromaDB** for vector storage and similarity search
* **Ollama (TinyLlama)** as the local LLM for answer generation

This notebook-based workflow provides a clear, linear learning experience with well-structured sections, comments, and outputs.

---


## üìÇ Project Structure

```
project/
‚îÇ‚îÄ‚îÄ notebook.ipynb            # Single consolidated notebook with the full RAG pipeline
‚îÇ‚îÄ‚îÄ sample.pdf                # Example PDF (optional)
‚îÇ‚îÄ‚îÄ README.md
```

**Note:**

There is **no `models/` folder** because the LLM is executed directly through  **Ollama** , which manages its own model storage.

---

## üß† Pipeline Overview

### **1. PDF Loading**

Extracts raw text from uploaded documents using PyPDF2.

### **2. Chunking**

Splits the extracted text into overlapping segments for better retrieval quality.

### **3. Embedding Generation**

Converts chunks into semantic vectors using `all-MiniLM-L6-v2`.

### **4. Vector Database (ChromaDB)**

Stores the text chunks + embeddings and performs cosine similarity search.

### **5. Query Handling**

User asks a natural-language question inside the notebook.

### **6. Retrieval**

Chroma returns the top-n relevant text chunks.

### **7. Prompt Construction**

Builds a controlled prompt that restricts LLM responses to retrieved context.

### **8. LLM Answering (Ollama)**

TinyLlama generates a grounded answer based purely on the provided context.

---

## üì¶ Installation

### **1. Install dependencies**

```bash
pip install PyPDF2 chromadb sentence-transformers
```

### **2. Install Ollama**

Download: [https://ollama.com/download](https://ollama.com/download)

Pull the TinyLlama model:

```bash
ollama pull tinyllama
```

---

## ‚ñ∂Ô∏è Usage Flow (inside the notebook)

### **Step 1 ‚Äî Upload or load a PDF**

Call the provided utility function to extract text.

### **Step 2 ‚Äî Split text into chunks**

Adjust chunk size and overlap as needed.

### **Step 3 ‚Äî Build Vector DB**

Embeddings + ChromaDB storage.

### **Step 4 ‚Äî Ask Questions**

Runs retrieval + LLM reasoning.

Example call:

```python
rag("What is the use of the open() function in Python?")
```

---

## üìò Example Output

```
QUESTION: What is the use of the open() function? 

ANSWER:
The open() function is used to open files and returns a file object that allows reading or writing.
```

---

## üôã Author

Created as an educational project to understand the core mechanics of RAG, embeddings, vector search, and local LLM reasoning.
